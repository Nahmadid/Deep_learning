{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOnOeKbokuI7kF7UHAEepsC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nahmadid/Deep_learning/blob/main/HW3_Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Shallow ReLU network**\n",
        "\n",
        "Write TensorFlow/PyTorch code to learn the oscillatory function $$f(x) = 5 + \\sum_{k=1}^{4} \\sin(kx), x \\in [-\\pi, 0)$$ and $$\\cos(10x), x \\in [0, \\pi]$$ \n",
        "\n",
        "using a shallow ReLU network (i.e., one hidden layer) and $200$ uniform data points in $[-\\pi, \\pi]$ (i.e., training dataset). Use networks of widths ${10, 30, 100, 300, 1000}$, and compute the $L_2$ relative errors of these networks on a testing dataset consisting of at least $500$ equi-spaced data points in $[-\\pi, \\pi]$. You can use the mean squared error (MSE) loss function for training the neural network model. Make sure that the network is trained well by choosing the appropriate learning rate and adequate number of epochs until convergence. Run your code at least $10$ times from different random weight initializations (referred to as seed in Python) and compute the mean and standard deviation of the prediction errors for each width. Plot the error versus the network width, and discuss what you observe.\n",
        "\n",
        "\n",
        "(a) Repeat Problem 3 using two hidden layers. Compare the new results and the results in Problem 3. Discuss your observations. Hint: Shallow networks vs Deep networks.\n",
        "\n",
        "(b) Repeat Problem 3 using 20 data points in $[-\\pi, \\pi]$. Compare the new results and the results in Problem 3. Discuss your observations. Hint: Estimation error."
      ],
      "metadata": {
        "id": "GBkPCOL5eqEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------\n",
        "\n",
        "#**Relative $L_2$  error of a network**\n",
        "\n",
        "The $L_2$ relative error measures the ratio of the $L_2$ norm of the difference between the predicted and true values of the function, to the $L_2$ norm of the true values. It is a normalized measure of the error, and it indicates how well the network is able to approximate the true function. If the $L_2$ relative error is small, then the network is able to closely approximate the true function, whereas if it is large, then the network is not able to capture the important features of the function.\n",
        "\n",
        "The $L_2$ relative error of a network can be computed as follows. Let $y_{true}$ be the true values of the function $f$ at the testing data points, and let $y_{pred}$ be the predicted values of the network at the same data points. Then, the $L_2$ relative error of the network is defined as:\n",
        "$$Relative- L_2  -error=\\frac{∥y_{pred}−y_{true}∥_2}{∥y_{true}∥_2}$$\n",
        "\n",
        "where $\\left\\lVert \\cdot \\right\\rVert_2$ denotes the $L_2$ norm."
      ],
      "metadata": {
        "id": "VdmVxdnogxef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-QiguVveZ17"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the oscillatory function\n",
        "def fun_x(x):\n",
        "    if x < 0.   :\n",
        "        f = 5.0 + tf.sin(x) + tf.sin(2.*x) + tf.sin(3.*x) + tf.sin(4.*x)+ tf.sin(5.*x)+ tf.sin(6.*x)\n",
        "    else:\n",
        "        f = tf.cos(10.*x)\n",
        "    return f\n",
        "\n",
        "\n",
        "def f(x):\n",
        "    condition1 = tf.less(x, 0.0)\n",
        "    condition2 = tf.greater_equal(x, 0.0)\n",
        "    y_true = 5.0 + tf.reduce_sum(tf.sin(tf.range(1, 5, dtype=tf.float32) * math.pi * x), axis=0)\n",
        "    y_false = tf.cos(10.0 * x)\n",
        "    return tf.where(condition1, y_true, y_false)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_plot = tf.linspace(-np.pi, np.pi, 100)\n",
        "y_plot = tf.map_fn(fun_x, x_plot)\n",
        "\n",
        "\n",
        "plt.plot(x_plot, y_plot)\n",
        "# plt.scatter(x_train, y_train, color=\"red\", label=\"Sampled points\")\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Plot of fun_x')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6rZmgKHGPQLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the oscillatory function f(x)\n",
        "# Define the oscillatory function]\n",
        "\n",
        "def f(x):\n",
        "    if x < 0.   :\n",
        "        f = 5.0 + tf.sin(x) + tf.sin(2.*x) + tf.sin(3.*x) + tf.sin(4.*x)+ tf.sin(5.*x)+ tf.sin(6.*x)\n",
        "    else:\n",
        "        f = tf.cos(10.*x)\n",
        "    return f\n",
        "\n",
        "# Define the neural network model\n",
        "def create_model(width):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(width, activation='relu', input_shape=(1,)),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer = tf.keras.optimizers.legacy.Adam(lr=0.001),\n",
        "                  loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Create the model and train it\n",
        "# Set random seed for TensorFlow backend\n",
        "tf.random.set_seed(40)\n",
        "x_train = tf.random.uniform(shape=[200, 1], minval=-np.pi, maxval=np.pi)\n",
        "y_train = tf.map_fn(f, x_train)\n",
        "\n",
        "# Generate testing data\n",
        "x_test = tf.linspace(-np.pi, np.pi, 500)[:, None]\n",
        "y_test = tf.map_fn(f, x_test)\n",
        "\n",
        "# Train the neural network model with different widths\n",
        "widths = [10, 30, 100, 300, 1000]\n",
        "num_runs = 10\n",
        "\n",
        "\n",
        "errors = np.zeros((len(widths), num_runs))\n",
        "\n",
        "for i, width in enumerate(widths):\n",
        "    for j in range(num_runs):\n",
        "\n",
        "        model = create_model(width)\n",
        "        model.fit(x_train, y_train, epochs=200, verbose=0)\n",
        "\n",
        "        # Compute the L2 relative error on the testing data\n",
        "        y_pred = model.predict(x_test)\n",
        "        error = tf.norm(y_pred - y_test,ord=2) / tf.norm(y_test,ord=2)\n",
        "        errors[i, j] = error\n",
        "\n",
        "# Compute the mean and standard deviation of the errors for each width\n",
        "mean_errors = np.mean(errors, axis=1)\n",
        "std_errors = np.std(errors, axis=1)\n",
        "\n",
        "# Plot the errors versus the network width\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.errorbar(widths, mean_errors, yerr=std_errors, fmt='o')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Network Width')\n",
        "plt.ylabel('L2 Relative Error')\n",
        "plt.title('Error vs Network Width')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "io87Sj776icm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "______________\n",
        "\n",
        "\n",
        "#**a) Deep Neural Network : two hidden layers**"
      ],
      "metadata": {
        "id": "k7E0yMxjXm_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the oscillatory function f(x)\n",
        "# Define the oscillatory function]\n",
        "\n",
        "def f(x):\n",
        "    if x < 0.   :\n",
        "        f = 5.0 + tf.sin(x) + tf.sin(2.*x) + tf.sin(3.*x) + tf.sin(4.*x)+ tf.sin(5.*x)+ tf.sin(6.*x)\n",
        "    else:\n",
        "        f = tf.cos(10.*x)\n",
        "    return f\n",
        "\n",
        "# Define the neural network model\n",
        "def create_model(width):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(width, activation='relu', input_shape=(1,)),\n",
        "        tf.keras.layers.Dense(width, activation='relu'),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer = tf.keras.optimizers.legacy.Adam(lr=0.001),\n",
        "                  loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Create the model and train it\n",
        "# Set random seed for TensorFlow backend\n",
        "tf.random.set_seed(40)\n",
        "x_train = tf.random.uniform(shape=[200, 1], minval=-np.pi, maxval=np.pi)\n",
        "y_train = tf.map_fn(f, x_train)\n",
        "\n",
        "# Generate testing data\n",
        "x_test = tf.linspace(-np.pi, np.pi, 500)[:, None]\n",
        "y_test = tf.map_fn(f, x_test)\n",
        "\n",
        "# Train the neural network model with different widths\n",
        "widths = [10, 30, 100, 300, 1000]\n",
        "num_runs = 10\n",
        "\n",
        "\n",
        "errors = np.zeros((len(widths), num_runs))\n",
        "\n",
        "for i, width in enumerate(widths):\n",
        "    for j in range(num_runs):\n",
        "\n",
        "        model = create_model(width)\n",
        "        model.fit(x_train, y_train, epochs=200, verbose=0)\n",
        "\n",
        "        # Compute the L2 relative error on the testing data\n",
        "        y_pred = model.predict(x_test)\n",
        "        error = tf.norm(y_pred - y_test,ord=2) / tf.norm(y_test,ord=2)\n",
        "        errors[i, j] = error\n",
        "\n",
        "# Compute the mean and standard deviation of the errors for each width\n",
        "mean_errors = np.mean(errors, axis=1)\n",
        "std_errors = np.std(errors, axis=1)\n",
        "\n",
        "# Plot the errors versus the network width\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.errorbar(widths, mean_errors, yerr=std_errors, fmt='o')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Network Width')\n",
        "plt.ylabel('L2 Relative Error')\n",
        "plt.title('Error vs Network Width')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3ognNtv4XFxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**b) Changing 200 data point to 20**"
      ],
      "metadata": {
        "id": "spM90evmbl3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the oscillatory function f(x)\n",
        "# Define the oscillatory function]\n",
        "\n",
        "def f(x):\n",
        "    if x < 0.   :\n",
        "        f = 5.0 + tf.sin(x) + tf.sin(2.*x) + tf.sin(3.*x) + tf.sin(4.*x)+ tf.sin(5.*x)+ tf.sin(6.*x)\n",
        "    else:\n",
        "        f = tf.cos(10.*x)\n",
        "    return f\n",
        "\n",
        "# Define the neural network model\n",
        "def create_model(width):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(width, activation='relu', input_shape=(1,)),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer = tf.keras.optimizers.legacy.Adam(lr=0.001),\n",
        "                  loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Create the model and train it\n",
        "# Set random seed for TensorFlow backend\n",
        "tf.random.set_seed(40)\n",
        "x_train = tf.random.uniform(shape=[20, 1], minval=-np.pi, maxval=np.pi)\n",
        "y_train = tf.map_fn(f, x_train)\n",
        "\n",
        "# Generate testing data\n",
        "x_test = tf.linspace(-np.pi, np.pi, 500)[:, None]\n",
        "y_test = tf.map_fn(f, x_test)\n",
        "\n",
        "# Train the neural network model with different widths\n",
        "widths = [10, 30, 100, 300, 1000]\n",
        "num_runs = 10\n",
        "\n",
        "\n",
        "errors = np.zeros((len(widths), num_runs))\n",
        "\n",
        "for i, width in enumerate(widths):\n",
        "    for j in range(num_runs):\n",
        "\n",
        "        model = create_model(width)\n",
        "        model.fit(x_train, y_train, epochs=200, verbose=0)\n",
        "\n",
        "        # Compute the L2 relative error on the testing data\n",
        "        y_pred = model.predict(x_test)\n",
        "        error = tf.norm(y_pred - y_test,ord=2) / tf.norm(y_test,ord=2)\n",
        "        errors[i, j] = error\n",
        "\n",
        "# Compute the mean and standard deviation of the errors for each width\n",
        "mean_errors = np.mean(errors, axis=1)\n",
        "std_errors = np.std(errors, axis=1)\n",
        "\n",
        "# Plot the errors versus the network width\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.errorbar(widths, mean_errors, yerr=std_errors, fmt='o')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Network Width')\n",
        "plt.ylabel('L2 Relative Error')\n",
        "plt.title('Error vs Network Width')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LUsJ6UMNb6lO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}